{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:19.924935Z",
     "start_time": "2025-04-06T15:09:19.920937Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各パス指定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:19.940212Z",
     "start_time": "2025-04-06T15:09:19.927609Z"
    }
   },
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "\n",
    "# Define the path where the model checkpoint will be saved (with proper path format)\n",
    "model_save_path = r'grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras'\n",
    "\n",
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "# You can now use these callbacks during model training:\n",
    "# model.fit(X_train, y_train, epochs=100, callbacks=[cp_callback, es_callback])\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類数設定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:19.972212Z",
     "start_time": "2025-04-06T15:09:19.957213Z"
    }
   },
   "source": "NUM_CLASSES = 6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.034998Z",
     "start_time": "2025-04-06T15:09:19.989213Z"
    }
   },
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.066452Z",
     "start_time": "2025-04-06T15:09:20.051453Z"
    }
   },
   "source": "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.097742Z",
     "start_time": "2025-04-06T15:09:20.082910Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.144382Z",
     "start_time": "2025-04-06T15:09:20.114353Z"
    }
   },
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.175513Z",
     "start_time": "2025-04-06T15:09:20.160513Z"
    }
   },
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │           \u001B[38;5;34m860\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │           \u001B[38;5;34m210\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │            \u001B[38;5;34m66\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,136\u001B[0m (4.44 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,136</span> (4.44 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.205753Z",
     "start_time": "2025-04-06T15:09:20.191513Z"
    }
   },
   "source": [
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:20.236662Z",
     "start_time": "2025-04-06T15:09:20.221662Z"
    }
   },
   "source": [
    "# モデルコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T15:09:41.367793Z",
     "start_time": "2025-04-06T15:09:20.252911Z"
    }
   },
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m9s\u001B[0m 349ms/step - accuracy: 0.0625 - loss: 2.0222\n",
      "Epoch 1: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.0773 - loss: 1.9791 - val_accuracy: 0.2572 - val_loss: 1.7389\n",
      "Epoch 2/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.2109 - loss: 1.7578\n",
      "Epoch 2: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.2342 - loss: 1.7574 - val_accuracy: 0.4045 - val_loss: 1.6213\n",
      "Epoch 3/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.3438 - loss: 1.6719\n",
      "Epoch 3: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3428 - loss: 1.6372 - val_accuracy: 0.4080 - val_loss: 1.5256\n",
      "Epoch 4/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.2734 - loss: 1.5504\n",
      "Epoch 4: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3527 - loss: 1.5440 - val_accuracy: 0.4568 - val_loss: 1.4422\n",
      "Epoch 5/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.4219 - loss: 1.5022\n",
      "Epoch 5: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4197 - loss: 1.4810 - val_accuracy: 0.5283 - val_loss: 1.3709\n",
      "Epoch 6/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.4062 - loss: 1.4479\n",
      "Epoch 6: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4316 - loss: 1.4221 - val_accuracy: 0.5388 - val_loss: 1.3062\n",
      "Epoch 7/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.3906 - loss: 1.4662\n",
      "Epoch 7: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4453 - loss: 1.3788 - val_accuracy: 0.5632 - val_loss: 1.2367\n",
      "Epoch 8/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.4688 - loss: 1.3573\n",
      "Epoch 8: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4599 - loss: 1.3226 - val_accuracy: 0.5859 - val_loss: 1.1669\n",
      "Epoch 9/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.4609 - loss: 1.2706\n",
      "Epoch 9: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4964 - loss: 1.2402 - val_accuracy: 0.6085 - val_loss: 1.0906\n",
      "Epoch 10/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.5234 - loss: 1.2300\n",
      "Epoch 10: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5111 - loss: 1.1958 - val_accuracy: 0.6173 - val_loss: 1.0209\n",
      "Epoch 11/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5391 - loss: 1.1013\n",
      "Epoch 11: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5180 - loss: 1.1655 - val_accuracy: 0.6460 - val_loss: 0.9645\n",
      "Epoch 12/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5625 - loss: 1.1091\n",
      "Epoch 12: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5249 - loss: 1.1306 - val_accuracy: 0.6879 - val_loss: 0.9138\n",
      "Epoch 13/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.5469 - loss: 1.0677\n",
      "Epoch 13: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5443 - loss: 1.1049 - val_accuracy: 0.7306 - val_loss: 0.8627\n",
      "Epoch 14/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.4766 - loss: 1.1436\n",
      "Epoch 14: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5409 - loss: 1.0798 - val_accuracy: 0.7384 - val_loss: 0.8244\n",
      "Epoch 15/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6328 - loss: 0.9176\n",
      "Epoch 15: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5931 - loss: 1.0027 - val_accuracy: 0.7515 - val_loss: 0.7856\n",
      "Epoch 16/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.5547 - loss: 1.0661\n",
      "Epoch 16: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5589 - loss: 1.0331 - val_accuracy: 0.7454 - val_loss: 0.7571\n",
      "Epoch 17/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6797 - loss: 0.9360\n",
      "Epoch 17: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5960 - loss: 0.9712 - val_accuracy: 0.7733 - val_loss: 0.7259\n",
      "Epoch 18/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6406 - loss: 0.9400\n",
      "Epoch 18: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5959 - loss: 0.9798 - val_accuracy: 0.7951 - val_loss: 0.6978\n",
      "Epoch 19/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.5547 - loss: 1.0904\n",
      "Epoch 19: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6070 - loss: 0.9680 - val_accuracy: 0.8073 - val_loss: 0.6693\n",
      "Epoch 20/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6641 - loss: 0.8054\n",
      "Epoch 20: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6246 - loss: 0.9202 - val_accuracy: 0.8152 - val_loss: 0.6566\n",
      "Epoch 21/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6328 - loss: 0.8654\n",
      "Epoch 21: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6358 - loss: 0.8990 - val_accuracy: 0.8204 - val_loss: 0.6292\n",
      "Epoch 22/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6484 - loss: 0.8508\n",
      "Epoch 22: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6250 - loss: 0.9057 - val_accuracy: 0.8387 - val_loss: 0.6121\n",
      "Epoch 23/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7031 - loss: 0.8454\n",
      "Epoch 23: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6503 - loss: 0.8875 - val_accuracy: 0.8370 - val_loss: 0.5931\n",
      "Epoch 24/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6328 - loss: 0.9711\n",
      "Epoch 24: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6464 - loss: 0.8882 - val_accuracy: 0.8439 - val_loss: 0.5758\n",
      "Epoch 25/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.5938 - loss: 1.0014\n",
      "Epoch 25: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6365 - loss: 0.9033 - val_accuracy: 0.8457 - val_loss: 0.5629\n",
      "Epoch 26/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6641 - loss: 0.8698\n",
      "Epoch 26: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6581 - loss: 0.8625 - val_accuracy: 0.8527 - val_loss: 0.5412\n",
      "Epoch 27/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6719 - loss: 0.7902\n",
      "Epoch 27: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6539 - loss: 0.8377 - val_accuracy: 0.8596 - val_loss: 0.5299\n",
      "Epoch 28/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6250 - loss: 0.8936\n",
      "Epoch 28: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6500 - loss: 0.8674 - val_accuracy: 0.8666 - val_loss: 0.5235\n",
      "Epoch 29/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7109 - loss: 0.7589\n",
      "Epoch 29: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6793 - loss: 0.8141 - val_accuracy: 0.8684 - val_loss: 0.5123\n",
      "Epoch 30/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6250 - loss: 0.8867\n",
      "Epoch 30: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6680 - loss: 0.8343 - val_accuracy: 0.8710 - val_loss: 0.4999\n",
      "Epoch 31/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6641 - loss: 0.9109\n",
      "Epoch 31: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6822 - loss: 0.8166 - val_accuracy: 0.8718 - val_loss: 0.4865\n",
      "Epoch 32/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.7266 - loss: 0.7984\n",
      "Epoch 32: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6785 - loss: 0.8232 - val_accuracy: 0.8666 - val_loss: 0.4868\n",
      "Epoch 33/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7109 - loss: 0.7727\n",
      "Epoch 33: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6978 - loss: 0.7922 - val_accuracy: 0.8701 - val_loss: 0.4778\n",
      "Epoch 34/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.7798\n",
      "Epoch 34: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7039 - loss: 0.7837 - val_accuracy: 0.8745 - val_loss: 0.4668\n",
      "Epoch 35/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6172 - loss: 0.8736\n",
      "Epoch 35: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6690 - loss: 0.8212 - val_accuracy: 0.8771 - val_loss: 0.4574\n",
      "Epoch 36/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m2s\u001B[0m 78ms/step - accuracy: 0.6953 - loss: 0.6885\n",
      "Epoch 36: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6829 - loss: 0.7829 - val_accuracy: 0.8858 - val_loss: 0.4441\n",
      "Epoch 37/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6406 - loss: 0.9419\n",
      "Epoch 37: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6929 - loss: 0.8005 - val_accuracy: 0.8788 - val_loss: 0.4418\n",
      "Epoch 38/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6905\n",
      "Epoch 38: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7031 - loss: 0.7612 - val_accuracy: 0.8788 - val_loss: 0.4334\n",
      "Epoch 39/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6797 - loss: 0.8182\n",
      "Epoch 39: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6940 - loss: 0.7918 - val_accuracy: 0.9024 - val_loss: 0.4231\n",
      "Epoch 40/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6406 - loss: 0.8019\n",
      "Epoch 40: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6839 - loss: 0.7694 - val_accuracy: 0.9032 - val_loss: 0.4128\n",
      "Epoch 41/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6641 - loss: 0.9634\n",
      "Epoch 41: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6943 - loss: 0.7897 - val_accuracy: 0.8858 - val_loss: 0.4184\n",
      "Epoch 42/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5583\n",
      "Epoch 42: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7236 - loss: 0.7259 - val_accuracy: 0.8963 - val_loss: 0.4082\n",
      "Epoch 43/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6077\n",
      "Epoch 43: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7190 - loss: 0.7192 - val_accuracy: 0.8971 - val_loss: 0.4047\n",
      "Epoch 44/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7500 - loss: 0.6481\n",
      "Epoch 44: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7204 - loss: 0.7467 - val_accuracy: 0.8875 - val_loss: 0.3997\n",
      "Epoch 45/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6998\n",
      "Epoch 45: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7163 - loss: 0.7389 - val_accuracy: 0.8945 - val_loss: 0.3954\n",
      "Epoch 46/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.7393\n",
      "Epoch 46: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7241 - loss: 0.7325 - val_accuracy: 0.9058 - val_loss: 0.3874\n",
      "Epoch 47/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6875 - loss: 0.7247\n",
      "Epoch 47: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7255 - loss: 0.7239 - val_accuracy: 0.9058 - val_loss: 0.3832\n",
      "Epoch 48/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7031 - loss: 0.8094\n",
      "Epoch 48: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7219 - loss: 0.7399 - val_accuracy: 0.8989 - val_loss: 0.3788\n",
      "Epoch 49/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7422 - loss: 0.7648\n",
      "Epoch 49: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7165 - loss: 0.7276 - val_accuracy: 0.9067 - val_loss: 0.3711\n",
      "Epoch 50/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6797 - loss: 0.7365\n",
      "Epoch 50: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7138 - loss: 0.7280 - val_accuracy: 0.9102 - val_loss: 0.3631\n",
      "Epoch 51/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7422 - loss: 0.6953\n",
      "Epoch 51: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7216 - loss: 0.7328 - val_accuracy: 0.9224 - val_loss: 0.3581\n",
      "Epoch 52/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.8351\n",
      "Epoch 52: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7119 - loss: 0.7598 - val_accuracy: 0.9198 - val_loss: 0.3601\n",
      "Epoch 53/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 32ms/step - accuracy: 0.6172 - loss: 0.8963\n",
      "Epoch 53: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7344 - loss: 0.7193 - val_accuracy: 0.9128 - val_loss: 0.3577\n",
      "Epoch 54/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6406 - loss: 0.9208\n",
      "Epoch 54: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7072 - loss: 0.7303 - val_accuracy: 0.9180 - val_loss: 0.3552\n",
      "Epoch 55/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6616\n",
      "Epoch 55: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7326 - loss: 0.6984 - val_accuracy: 0.9250 - val_loss: 0.3499\n",
      "Epoch 56/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6953 - loss: 0.7095\n",
      "Epoch 56: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7316 - loss: 0.7040 - val_accuracy: 0.9085 - val_loss: 0.3502\n",
      "Epoch 57/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7031 - loss: 0.6687\n",
      "Epoch 57: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7242 - loss: 0.7037 - val_accuracy: 0.9276 - val_loss: 0.3347\n",
      "Epoch 58/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5950\n",
      "Epoch 58: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7249 - loss: 0.7049 - val_accuracy: 0.9224 - val_loss: 0.3354\n",
      "Epoch 59/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6875 - loss: 0.7105\n",
      "Epoch 59: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7245 - loss: 0.6898 - val_accuracy: 0.9076 - val_loss: 0.3374\n",
      "Epoch 60/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.7135\n",
      "Epoch 60: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7400 - loss: 0.6977 - val_accuracy: 0.9224 - val_loss: 0.3325\n",
      "Epoch 61/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6953 - loss: 0.7222\n",
      "Epoch 61: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7408 - loss: 0.6716 - val_accuracy: 0.9303 - val_loss: 0.3204\n",
      "Epoch 62/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6953 - loss: 0.7629\n",
      "Epoch 62: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7210 - loss: 0.7136 - val_accuracy: 0.9215 - val_loss: 0.3329\n",
      "Epoch 63/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6797 - loss: 0.7956\n",
      "Epoch 63: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7465 - loss: 0.6779 - val_accuracy: 0.9346 - val_loss: 0.3217\n",
      "Epoch 64/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6471\n",
      "Epoch 64: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7539 - loss: 0.6725 - val_accuracy: 0.9381 - val_loss: 0.3256\n",
      "Epoch 65/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7500 - loss: 0.7264\n",
      "Epoch 65: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7441 - loss: 0.7074 - val_accuracy: 0.9364 - val_loss: 0.3285\n",
      "Epoch 66/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.7449\n",
      "Epoch 66: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7395 - loss: 0.6798 - val_accuracy: 0.9364 - val_loss: 0.3282\n",
      "Epoch 67/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7109 - loss: 0.7946\n",
      "Epoch 67: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7326 - loss: 0.6987 - val_accuracy: 0.9268 - val_loss: 0.3246\n",
      "Epoch 68/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.7638\n",
      "Epoch 68: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7447 - loss: 0.6932 - val_accuracy: 0.9381 - val_loss: 0.3191\n",
      "Epoch 69/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.6797 - loss: 0.7415\n",
      "Epoch 69: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7433 - loss: 0.6551 - val_accuracy: 0.9390 - val_loss: 0.3144\n",
      "Epoch 70/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.6711\n",
      "Epoch 70: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7574 - loss: 0.6524 - val_accuracy: 0.9329 - val_loss: 0.3120\n",
      "Epoch 71/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.6528\n",
      "Epoch 71: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7535 - loss: 0.6642 - val_accuracy: 0.9355 - val_loss: 0.3092\n",
      "Epoch 72/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.7630\n",
      "Epoch 72: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7539 - loss: 0.6652 - val_accuracy: 0.9416 - val_loss: 0.3094\n",
      "Epoch 73/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7422 - loss: 0.6481\n",
      "Epoch 73: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7373 - loss: 0.6876 - val_accuracy: 0.9337 - val_loss: 0.3103\n",
      "Epoch 74/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.7056\n",
      "Epoch 74: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7349 - loss: 0.6759 - val_accuracy: 0.9425 - val_loss: 0.3064\n",
      "Epoch 75/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.6441\n",
      "Epoch 75: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7595 - loss: 0.6500 - val_accuracy: 0.9547 - val_loss: 0.2963\n",
      "Epoch 76/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5595\n",
      "Epoch 76: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7595 - loss: 0.6330 - val_accuracy: 0.9538 - val_loss: 0.2915\n",
      "Epoch 77/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.5877\n",
      "Epoch 77: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7503 - loss: 0.6423 - val_accuracy: 0.9529 - val_loss: 0.2879\n",
      "Epoch 78/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6448\n",
      "Epoch 78: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7581 - loss: 0.6498 - val_accuracy: 0.9582 - val_loss: 0.2861\n",
      "Epoch 79/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.7513\n",
      "Epoch 79: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7631 - loss: 0.6580 - val_accuracy: 0.9529 - val_loss: 0.2934\n",
      "Epoch 80/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.7422 - loss: 0.6293\n",
      "Epoch 80: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7506 - loss: 0.6667 - val_accuracy: 0.9390 - val_loss: 0.2949\n",
      "Epoch 81/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6252\n",
      "Epoch 81: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7688 - loss: 0.6295 - val_accuracy: 0.9538 - val_loss: 0.2898\n",
      "Epoch 82/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7188 - loss: 0.6518\n",
      "Epoch 82: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7460 - loss: 0.6422 - val_accuracy: 0.9573 - val_loss: 0.2781\n",
      "Epoch 83/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.6937\n",
      "Epoch 83: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7439 - loss: 0.6609 - val_accuracy: 0.9503 - val_loss: 0.2829\n",
      "Epoch 84/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5661\n",
      "Epoch 84: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7619 - loss: 0.6163 - val_accuracy: 0.9660 - val_loss: 0.2658\n",
      "Epoch 85/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.5787\n",
      "Epoch 85: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7615 - loss: 0.6401 - val_accuracy: 0.9660 - val_loss: 0.2697\n",
      "Epoch 86/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.5988\n",
      "Epoch 86: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7484 - loss: 0.6443 - val_accuracy: 0.9590 - val_loss: 0.2783\n",
      "Epoch 87/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7500 - loss: 0.6345\n",
      "Epoch 87: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7585 - loss: 0.6592 - val_accuracy: 0.9538 - val_loss: 0.2806\n",
      "Epoch 88/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.7078\n",
      "Epoch 88: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7586 - loss: 0.6501 - val_accuracy: 0.9582 - val_loss: 0.2738\n",
      "Epoch 89/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6031\n",
      "Epoch 89: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7550 - loss: 0.6698 - val_accuracy: 0.9564 - val_loss: 0.2767\n",
      "Epoch 90/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6736\n",
      "Epoch 90: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7487 - loss: 0.6666 - val_accuracy: 0.9564 - val_loss: 0.2798\n",
      "Epoch 91/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7344 - loss: 0.6264\n",
      "Epoch 91: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7578 - loss: 0.6019 - val_accuracy: 0.9520 - val_loss: 0.2776\n",
      "Epoch 92/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.6732\n",
      "Epoch 92: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7549 - loss: 0.6583 - val_accuracy: 0.9643 - val_loss: 0.2647\n",
      "Epoch 93/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.5483\n",
      "Epoch 93: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7742 - loss: 0.6153 - val_accuracy: 0.9547 - val_loss: 0.2792\n",
      "Epoch 94/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.7038\n",
      "Epoch 94: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7667 - loss: 0.6522 - val_accuracy: 0.9538 - val_loss: 0.2709\n",
      "Epoch 95/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6660\n",
      "Epoch 95: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7662 - loss: 0.6430 - val_accuracy: 0.9582 - val_loss: 0.2694\n",
      "Epoch 96/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5784\n",
      "Epoch 96: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7763 - loss: 0.6094 - val_accuracy: 0.9625 - val_loss: 0.2579\n",
      "Epoch 97/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.7181\n",
      "Epoch 97: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7669 - loss: 0.6159 - val_accuracy: 0.9564 - val_loss: 0.2700\n",
      "Epoch 98/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.5916\n",
      "Epoch 98: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7673 - loss: 0.6355 - val_accuracy: 0.9590 - val_loss: 0.2620\n",
      "Epoch 99/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5287\n",
      "Epoch 99: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7845 - loss: 0.6140 - val_accuracy: 0.9608 - val_loss: 0.2598\n",
      "Epoch 100/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6493\n",
      "Epoch 100: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7731 - loss: 0.6279 - val_accuracy: 0.9582 - val_loss: 0.2605\n",
      "Epoch 101/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7109 - loss: 0.8103\n",
      "Epoch 101: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7497 - loss: 0.6695 - val_accuracy: 0.9582 - val_loss: 0.2614\n",
      "Epoch 102/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.6126\n",
      "Epoch 102: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7838 - loss: 0.6041 - val_accuracy: 0.9608 - val_loss: 0.2557\n",
      "Epoch 103/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7422 - loss: 0.7509\n",
      "Epoch 103: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7654 - loss: 0.6345 - val_accuracy: 0.9625 - val_loss: 0.2576\n",
      "Epoch 104/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.5592\n",
      "Epoch 104: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7920 - loss: 0.5897 - val_accuracy: 0.9634 - val_loss: 0.2491\n",
      "Epoch 105/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8281 - loss: 0.4830\n",
      "Epoch 105: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7775 - loss: 0.6139 - val_accuracy: 0.9599 - val_loss: 0.2533\n",
      "Epoch 106/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.7242\n",
      "Epoch 106: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7698 - loss: 0.6416 - val_accuracy: 0.9625 - val_loss: 0.2566\n",
      "Epoch 107/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5654\n",
      "Epoch 107: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7763 - loss: 0.6012 - val_accuracy: 0.9634 - val_loss: 0.2449\n",
      "Epoch 108/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.6409\n",
      "Epoch 108: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7752 - loss: 0.6369 - val_accuracy: 0.9651 - val_loss: 0.2491\n",
      "Epoch 109/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8438 - loss: 0.4575\n",
      "Epoch 109: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7760 - loss: 0.6142 - val_accuracy: 0.9704 - val_loss: 0.2486\n",
      "Epoch 110/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8203 - loss: 0.5242\n",
      "Epoch 110: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7705 - loss: 0.6172 - val_accuracy: 0.9686 - val_loss: 0.2460\n",
      "Epoch 111/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8281 - loss: 0.5177\n",
      "Epoch 111: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7850 - loss: 0.5857 - val_accuracy: 0.9669 - val_loss: 0.2394\n",
      "Epoch 112/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5381\n",
      "Epoch 112: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7873 - loss: 0.5639 - val_accuracy: 0.9721 - val_loss: 0.2352\n",
      "Epoch 113/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.5871\n",
      "Epoch 113: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7702 - loss: 0.6352 - val_accuracy: 0.9634 - val_loss: 0.2433\n",
      "Epoch 114/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5980\n",
      "Epoch 114: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7894 - loss: 0.5816 - val_accuracy: 0.9669 - val_loss: 0.2301\n",
      "Epoch 115/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.5700\n",
      "Epoch 115: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7765 - loss: 0.5956 - val_accuracy: 0.9721 - val_loss: 0.2284\n",
      "Epoch 116/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5322\n",
      "Epoch 116: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7893 - loss: 0.5923 - val_accuracy: 0.9616 - val_loss: 0.2418\n",
      "Epoch 117/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7344 - loss: 0.6899\n",
      "Epoch 117: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7638 - loss: 0.6227 - val_accuracy: 0.9555 - val_loss: 0.2467\n",
      "Epoch 118/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7812 - loss: 0.5865\n",
      "Epoch 118: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7773 - loss: 0.5944 - val_accuracy: 0.9677 - val_loss: 0.2354\n",
      "Epoch 119/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5463\n",
      "Epoch 119: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7695 - loss: 0.6114 - val_accuracy: 0.9677 - val_loss: 0.2298\n",
      "Epoch 120/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.6110\n",
      "Epoch 120: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7693 - loss: 0.6375 - val_accuracy: 0.9686 - val_loss: 0.2340\n",
      "Epoch 121/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.5587\n",
      "Epoch 121: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7871 - loss: 0.5946 - val_accuracy: 0.9643 - val_loss: 0.2286\n",
      "Epoch 122/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.7024\n",
      "Epoch 122: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7688 - loss: 0.6131 - val_accuracy: 0.9677 - val_loss: 0.2359\n",
      "Epoch 123/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5494\n",
      "Epoch 123: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7797 - loss: 0.5854 - val_accuracy: 0.9651 - val_loss: 0.2450\n",
      "Epoch 124/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5866\n",
      "Epoch 124: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7832 - loss: 0.5969 - val_accuracy: 0.9651 - val_loss: 0.2334\n",
      "Epoch 125/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8359 - loss: 0.4527\n",
      "Epoch 125: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7991 - loss: 0.5792 - val_accuracy: 0.9704 - val_loss: 0.2312\n",
      "Epoch 126/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7188 - loss: 0.6996\n",
      "Epoch 126: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7721 - loss: 0.6165 - val_accuracy: 0.9669 - val_loss: 0.2352\n",
      "Epoch 127/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.6257\n",
      "Epoch 127: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7860 - loss: 0.5916 - val_accuracy: 0.9695 - val_loss: 0.2305\n",
      "Epoch 128/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.7016\n",
      "Epoch 128: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7817 - loss: 0.5928 - val_accuracy: 0.9643 - val_loss: 0.2326\n",
      "Epoch 129/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6301\n",
      "Epoch 129: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7720 - loss: 0.6227 - val_accuracy: 0.9634 - val_loss: 0.2300\n",
      "Epoch 130/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5741\n",
      "Epoch 130: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7778 - loss: 0.6205 - val_accuracy: 0.9643 - val_loss: 0.2281\n",
      "Epoch 131/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5895\n",
      "Epoch 131: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7834 - loss: 0.5788 - val_accuracy: 0.9686 - val_loss: 0.2241\n",
      "Epoch 132/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.6158\n",
      "Epoch 132: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7946 - loss: 0.5877 - val_accuracy: 0.9686 - val_loss: 0.2217\n",
      "Epoch 133/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5522\n",
      "Epoch 133: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7835 - loss: 0.6043 - val_accuracy: 0.9669 - val_loss: 0.2245\n",
      "Epoch 134/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.6516\n",
      "Epoch 134: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7921 - loss: 0.5971 - val_accuracy: 0.9677 - val_loss: 0.2298\n",
      "Epoch 135/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5581\n",
      "Epoch 135: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7910 - loss: 0.5781 - val_accuracy: 0.9686 - val_loss: 0.2268\n",
      "Epoch 136/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5913\n",
      "Epoch 136: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7888 - loss: 0.5802 - val_accuracy: 0.9651 - val_loss: 0.2260\n",
      "Epoch 137/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5728\n",
      "Epoch 137: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7920 - loss: 0.5850 - val_accuracy: 0.9712 - val_loss: 0.2189\n",
      "Epoch 138/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6518\n",
      "Epoch 138: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7768 - loss: 0.6069 - val_accuracy: 0.9616 - val_loss: 0.2280\n",
      "Epoch 139/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.6347\n",
      "Epoch 139: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7782 - loss: 0.6206 - val_accuracy: 0.9677 - val_loss: 0.2307\n",
      "Epoch 140/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5122\n",
      "Epoch 140: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7862 - loss: 0.5758 - val_accuracy: 0.9721 - val_loss: 0.2253\n",
      "Epoch 141/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.5543\n",
      "Epoch 141: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7889 - loss: 0.5646 - val_accuracy: 0.9686 - val_loss: 0.2281\n",
      "Epoch 142/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5501\n",
      "Epoch 142: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7812 - loss: 0.5992 - val_accuracy: 0.9721 - val_loss: 0.2231\n",
      "Epoch 143/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5760\n",
      "Epoch 143: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7969 - loss: 0.5820 - val_accuracy: 0.9634 - val_loss: 0.2256\n",
      "Epoch 144/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7812 - loss: 0.6266\n",
      "Epoch 144: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7913 - loss: 0.5928 - val_accuracy: 0.9669 - val_loss: 0.2318\n",
      "Epoch 145/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.5853\n",
      "Epoch 145: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7865 - loss: 0.5908 - val_accuracy: 0.9677 - val_loss: 0.2295\n",
      "Epoch 146/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5663\n",
      "Epoch 146: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7903 - loss: 0.5989 - val_accuracy: 0.9677 - val_loss: 0.2338\n",
      "Epoch 147/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6402\n",
      "Epoch 147: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8003 - loss: 0.5763 - val_accuracy: 0.9677 - val_loss: 0.2298\n",
      "Epoch 148/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7891 - loss: 0.5296\n",
      "Epoch 148: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7952 - loss: 0.5684 - val_accuracy: 0.9686 - val_loss: 0.2288\n",
      "Epoch 149/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.5079\n",
      "Epoch 149: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8098 - loss: 0.5733 - val_accuracy: 0.9660 - val_loss: 0.2221\n",
      "Epoch 150/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5778\n",
      "Epoch 150: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7880 - loss: 0.5866 - val_accuracy: 0.9712 - val_loss: 0.2232\n",
      "Epoch 151/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5207\n",
      "Epoch 151: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7791 - loss: 0.5876 - val_accuracy: 0.9712 - val_loss: 0.2222\n",
      "Epoch 152/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.5847\n",
      "Epoch 152: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7905 - loss: 0.5643 - val_accuracy: 0.9721 - val_loss: 0.2134\n",
      "Epoch 153/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.5806\n",
      "Epoch 153: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7878 - loss: 0.5723 - val_accuracy: 0.9730 - val_loss: 0.2110\n",
      "Epoch 154/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.4988\n",
      "Epoch 154: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7963 - loss: 0.5629 - val_accuracy: 0.9704 - val_loss: 0.2186\n",
      "Epoch 155/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.5500\n",
      "Epoch 155: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7978 - loss: 0.5776 - val_accuracy: 0.9730 - val_loss: 0.2155\n",
      "Epoch 156/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8203 - loss: 0.4869\n",
      "Epoch 156: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7828 - loss: 0.6050 - val_accuracy: 0.9695 - val_loss: 0.2208\n",
      "Epoch 157/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.7578 - loss: 0.6877\n",
      "Epoch 157: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7981 - loss: 0.5727 - val_accuracy: 0.9695 - val_loss: 0.2170\n",
      "Epoch 158/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5425\n",
      "Epoch 158: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7884 - loss: 0.5843 - val_accuracy: 0.9704 - val_loss: 0.2182\n",
      "Epoch 159/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5415\n",
      "Epoch 159: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7896 - loss: 0.5767 - val_accuracy: 0.9721 - val_loss: 0.2196\n",
      "Epoch 160/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7266 - loss: 0.6685\n",
      "Epoch 160: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7913 - loss: 0.5859 - val_accuracy: 0.9738 - val_loss: 0.2158\n",
      "Epoch 161/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5529\n",
      "Epoch 161: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7805 - loss: 0.5922 - val_accuracy: 0.9695 - val_loss: 0.2194\n",
      "Epoch 162/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8203 - loss: 0.4853\n",
      "Epoch 162: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7974 - loss: 0.5612 - val_accuracy: 0.9712 - val_loss: 0.2151\n",
      "Epoch 163/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.5796\n",
      "Epoch 163: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8022 - loss: 0.5668 - val_accuracy: 0.9677 - val_loss: 0.2146\n",
      "Epoch 164/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6269\n",
      "Epoch 164: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7851 - loss: 0.5766 - val_accuracy: 0.9704 - val_loss: 0.2156\n",
      "Epoch 165/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7656 - loss: 0.6097\n",
      "Epoch 165: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7952 - loss: 0.5745 - val_accuracy: 0.9721 - val_loss: 0.2215\n",
      "Epoch 166/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8516 - loss: 0.4536\n",
      "Epoch 166: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8142 - loss: 0.5531 - val_accuracy: 0.9686 - val_loss: 0.2156\n",
      "Epoch 167/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8438 - loss: 0.5172\n",
      "Epoch 167: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8020 - loss: 0.5605 - val_accuracy: 0.9730 - val_loss: 0.2079\n",
      "Epoch 168/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.8203 - loss: 0.5927\n",
      "Epoch 168: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8056 - loss: 0.5605 - val_accuracy: 0.9695 - val_loss: 0.2079\n",
      "Epoch 169/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.8047 - loss: 0.6494\n",
      "Epoch 169: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7937 - loss: 0.5838 - val_accuracy: 0.9695 - val_loss: 0.2152\n",
      "Epoch 170/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5463\n",
      "Epoch 170: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8001 - loss: 0.5745 - val_accuracy: 0.9669 - val_loss: 0.2130\n",
      "Epoch 171/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8203 - loss: 0.5899\n",
      "Epoch 171: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7972 - loss: 0.5781 - val_accuracy: 0.9677 - val_loss: 0.2113\n",
      "Epoch 172/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8438 - loss: 0.4425\n",
      "Epoch 172: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7996 - loss: 0.5530 - val_accuracy: 0.9695 - val_loss: 0.2084\n",
      "Epoch 173/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5852\n",
      "Epoch 173: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7948 - loss: 0.5861 - val_accuracy: 0.9721 - val_loss: 0.2094\n",
      "Epoch 174/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7656 - loss: 0.6026\n",
      "Epoch 174: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7899 - loss: 0.5752 - val_accuracy: 0.9730 - val_loss: 0.2062\n",
      "Epoch 175/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.8203 - loss: 0.4570\n",
      "Epoch 175: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8049 - loss: 0.5561 - val_accuracy: 0.9730 - val_loss: 0.2061\n",
      "Epoch 176/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8438 - loss: 0.4919\n",
      "Epoch 176: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8105 - loss: 0.5427 - val_accuracy: 0.9747 - val_loss: 0.1993\n",
      "Epoch 177/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.4823\n",
      "Epoch 177: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7987 - loss: 0.5707 - val_accuracy: 0.9721 - val_loss: 0.2064\n",
      "Epoch 178/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.4889\n",
      "Epoch 178: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7993 - loss: 0.5592 - val_accuracy: 0.9747 - val_loss: 0.2077\n",
      "Epoch 179/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5535\n",
      "Epoch 179: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7976 - loss: 0.5620 - val_accuracy: 0.9738 - val_loss: 0.2032\n",
      "Epoch 180/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5824\n",
      "Epoch 180: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7998 - loss: 0.5565 - val_accuracy: 0.9704 - val_loss: 0.2099\n",
      "Epoch 181/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.4362\n",
      "Epoch 181: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8115 - loss: 0.5441 - val_accuracy: 0.9730 - val_loss: 0.2129\n",
      "Epoch 182/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8672 - loss: 0.4783\n",
      "Epoch 182: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8001 - loss: 0.5569 - val_accuracy: 0.9721 - val_loss: 0.2149\n",
      "Epoch 183/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5520\n",
      "Epoch 183: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7988 - loss: 0.5608 - val_accuracy: 0.9712 - val_loss: 0.2111\n",
      "Epoch 184/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8516 - loss: 0.4611\n",
      "Epoch 184: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8002 - loss: 0.5642 - val_accuracy: 0.9712 - val_loss: 0.2075\n",
      "Epoch 185/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5170\n",
      "Epoch 185: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7906 - loss: 0.5788 - val_accuracy: 0.9686 - val_loss: 0.2076\n",
      "Epoch 186/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.6221\n",
      "Epoch 186: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7828 - loss: 0.5824 - val_accuracy: 0.9738 - val_loss: 0.2102\n",
      "Epoch 187/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5062\n",
      "Epoch 187: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7874 - loss: 0.5760 - val_accuracy: 0.9738 - val_loss: 0.2071\n",
      "Epoch 188/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.7137\n",
      "Epoch 188: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7996 - loss: 0.5999 - val_accuracy: 0.9765 - val_loss: 0.1995\n",
      "Epoch 189/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7188 - loss: 0.6855\n",
      "Epoch 189: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7843 - loss: 0.5840 - val_accuracy: 0.9738 - val_loss: 0.2051\n",
      "Epoch 190/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8594 - loss: 0.4604\n",
      "Epoch 190: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8124 - loss: 0.5375 - val_accuracy: 0.9730 - val_loss: 0.2084\n",
      "Epoch 191/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.8359 - loss: 0.5506\n",
      "Epoch 191: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8139 - loss: 0.5482 - val_accuracy: 0.9730 - val_loss: 0.2033\n",
      "Epoch 192/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5925\n",
      "Epoch 192: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8040 - loss: 0.5540 - val_accuracy: 0.9738 - val_loss: 0.2073\n",
      "Epoch 193/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7500 - loss: 0.5939\n",
      "Epoch 193: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7946 - loss: 0.5501 - val_accuracy: 0.9730 - val_loss: 0.2028\n",
      "Epoch 194/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8047 - loss: 0.5613\n",
      "Epoch 194: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8188 - loss: 0.5278 - val_accuracy: 0.9747 - val_loss: 0.1980\n",
      "Epoch 195/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.4923\n",
      "Epoch 195: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7967 - loss: 0.5564 - val_accuracy: 0.9704 - val_loss: 0.2058\n",
      "Epoch 196/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.4770\n",
      "Epoch 196: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8013 - loss: 0.5379 - val_accuracy: 0.9738 - val_loss: 0.2090\n",
      "Epoch 197/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7969 - loss: 0.5925\n",
      "Epoch 197: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8057 - loss: 0.5529 - val_accuracy: 0.9738 - val_loss: 0.2016\n",
      "Epoch 198/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5831\n",
      "Epoch 198: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.5349 - val_accuracy: 0.9738 - val_loss: 0.1982\n",
      "Epoch 199/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.4914\n",
      "Epoch 199: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8042 - loss: 0.5477 - val_accuracy: 0.9747 - val_loss: 0.2021\n",
      "Epoch 200/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.4765\n",
      "Epoch 200: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.5181 - val_accuracy: 0.9756 - val_loss: 0.1932\n",
      "Epoch 201/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.4399\n",
      "Epoch 201: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7886 - loss: 0.5582 - val_accuracy: 0.9686 - val_loss: 0.2054\n",
      "Epoch 202/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8438 - loss: 0.5077\n",
      "Epoch 202: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7797 - loss: 0.6027 - val_accuracy: 0.9712 - val_loss: 0.2037\n",
      "Epoch 203/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.5925\n",
      "Epoch 203: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7958 - loss: 0.5549 - val_accuracy: 0.9721 - val_loss: 0.2013\n",
      "Epoch 204/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.5804\n",
      "Epoch 204: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8199 - loss: 0.5386 - val_accuracy: 0.9712 - val_loss: 0.2010\n",
      "Epoch 205/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.5225\n",
      "Epoch 205: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7965 - loss: 0.5663 - val_accuracy: 0.9721 - val_loss: 0.2024\n",
      "Epoch 206/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7812 - loss: 0.5693\n",
      "Epoch 206: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8049 - loss: 0.5511 - val_accuracy: 0.9704 - val_loss: 0.2048\n",
      "Epoch 207/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.4656\n",
      "Epoch 207: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8151 - loss: 0.5216 - val_accuracy: 0.9756 - val_loss: 0.1941\n",
      "Epoch 208/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8281 - loss: 0.4725\n",
      "Epoch 208: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8062 - loss: 0.5431 - val_accuracy: 0.9782 - val_loss: 0.1922\n",
      "Epoch 209/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5688\n",
      "Epoch 209: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7842 - loss: 0.5888 - val_accuracy: 0.9686 - val_loss: 0.2042\n",
      "Epoch 210/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8672 - loss: 0.3775\n",
      "Epoch 210: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8099 - loss: 0.5346 - val_accuracy: 0.9712 - val_loss: 0.2106\n",
      "Epoch 211/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5993\n",
      "Epoch 211: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8200 - loss: 0.5330 - val_accuracy: 0.9669 - val_loss: 0.2067\n",
      "Epoch 212/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8828 - loss: 0.3879\n",
      "Epoch 212: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8207 - loss: 0.5247 - val_accuracy: 0.9686 - val_loss: 0.2023\n",
      "Epoch 213/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.5587\n",
      "Epoch 213: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8088 - loss: 0.5496 - val_accuracy: 0.9712 - val_loss: 0.2021\n",
      "Epoch 214/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8359 - loss: 0.4606\n",
      "Epoch 214: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8054 - loss: 0.5312 - val_accuracy: 0.9756 - val_loss: 0.1850\n",
      "Epoch 215/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7578 - loss: 0.6467\n",
      "Epoch 215: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7851 - loss: 0.5892 - val_accuracy: 0.9730 - val_loss: 0.1966\n",
      "Epoch 216/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7109 - loss: 0.7488\n",
      "Epoch 216: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7757 - loss: 0.6108 - val_accuracy: 0.9669 - val_loss: 0.2035\n",
      "Epoch 217/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.8125 - loss: 0.5911\n",
      "Epoch 217: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8046 - loss: 0.5562 - val_accuracy: 0.9704 - val_loss: 0.1994\n",
      "Epoch 218/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8750 - loss: 0.4520\n",
      "Epoch 218: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8013 - loss: 0.5472 - val_accuracy: 0.9712 - val_loss: 0.2020\n",
      "Epoch 219/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8047 - loss: 0.5736\n",
      "Epoch 219: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8099 - loss: 0.5462 - val_accuracy: 0.9686 - val_loss: 0.2029\n",
      "Epoch 220/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8281 - loss: 0.4038\n",
      "Epoch 220: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8003 - loss: 0.5340 - val_accuracy: 0.9773 - val_loss: 0.1938\n",
      "Epoch 221/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8359 - loss: 0.5431\n",
      "Epoch 221: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7956 - loss: 0.5731 - val_accuracy: 0.9695 - val_loss: 0.2046\n",
      "Epoch 222/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5285\n",
      "Epoch 222: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7972 - loss: 0.5681 - val_accuracy: 0.9669 - val_loss: 0.2058\n",
      "Epoch 223/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7969 - loss: 0.4823\n",
      "Epoch 223: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8122 - loss: 0.5155 - val_accuracy: 0.9712 - val_loss: 0.1965\n",
      "Epoch 224/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8203 - loss: 0.5541\n",
      "Epoch 224: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8056 - loss: 0.5680 - val_accuracy: 0.9677 - val_loss: 0.1991\n",
      "Epoch 225/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8125 - loss: 0.5274\n",
      "Epoch 225: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8088 - loss: 0.5392 - val_accuracy: 0.9747 - val_loss: 0.1946\n",
      "Epoch 226/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7812 - loss: 0.6112\n",
      "Epoch 226: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7956 - loss: 0.5639 - val_accuracy: 0.9695 - val_loss: 0.2017\n",
      "Epoch 227/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8125 - loss: 0.5353\n",
      "Epoch 227: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7965 - loss: 0.5624 - val_accuracy: 0.9660 - val_loss: 0.2044\n",
      "Epoch 228/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7734 - loss: 0.6194\n",
      "Epoch 228: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8034 - loss: 0.5514 - val_accuracy: 0.9730 - val_loss: 0.1976\n",
      "Epoch 229/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8750 - loss: 0.4060\n",
      "Epoch 229: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.5072 - val_accuracy: 0.9695 - val_loss: 0.2029\n",
      "Epoch 230/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8359 - loss: 0.5064\n",
      "Epoch 230: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8003 - loss: 0.5466 - val_accuracy: 0.9738 - val_loss: 0.1990\n",
      "Epoch 231/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7891 - loss: 0.5423\n",
      "Epoch 231: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.7937 - loss: 0.5568 - val_accuracy: 0.9712 - val_loss: 0.1984\n",
      "Epoch 232/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.7578 - loss: 0.6099\n",
      "Epoch 232: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8091 - loss: 0.5300 - val_accuracy: 0.9747 - val_loss: 0.1860\n",
      "Epoch 233/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.7734 - loss: 0.6119\n",
      "Epoch 233: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8058 - loss: 0.5446 - val_accuracy: 0.9730 - val_loss: 0.1926\n",
      "Epoch 234/1000\n",
      "\u001B[1m 1/27\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8438 - loss: 0.5648\n",
      "Epoch 234: saving model to grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\model\\keypoint_classifier\\keypoint_classifier.keras\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.8114 - loss: 0.5478 - val_accuracy: 0.9677 - val_loss: 0.2020\n",
      "Epoch 234: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x260ca15e890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:26.710697Z",
     "start_time": "2025-04-06T15:10:26.649490Z"
    }
   },
   "source": [
    "# モデル評価\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9692 - loss: 0.2051 \n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:26.772708Z",
     "start_time": "2025-04-06T15:10:26.726968Z"
    }
   },
   "source": [
    "# 保存したモデルのロード\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:26.850307Z",
     "start_time": "2025-04-06T15:10:26.789232Z"
    }
   },
   "source": [
    "# 推論テスト\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step\n",
      "[4.1954637e-02 7.1722388e-02 8.7705678e-01 8.6475220e-03 5.7122717e-04\n",
      " 4.7421348e-05]\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.038162Z",
     "start_time": "2025-04-06T15:10:26.866818Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m36/36\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 680us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       408\n",
      "           1       1.00      0.89      0.94       237\n",
      "           2       0.91      0.99      0.95       329\n",
      "           3       1.00      1.00      1.00        70\n",
      "           4       1.00      0.95      0.98        64\n",
      "           5       1.00      0.97      0.99        39\n",
      "\n",
      "    accuracy                           0.97      1147\n",
      "   macro avg       0.98      0.97      0.97      1147\n",
      "weighted avg       0.97      0.97      0.97      1147\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Lite用のモデルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.242924Z",
     "start_time": "2025-04-06T15:10:27.054441Z"
    }
   },
   "source": [
    "# モデルを変換(量子化)\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmpx0n8ggq1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\grimm\\AppData\\Local\\Temp\\tmpx0n8ggq1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\grimm\\AppData\\Local\\Temp\\tmpx0n8ggq1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2614704504176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2614869353648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2614704496960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2614869345904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2614869344496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2614871934928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6644"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.274132Z",
     "start_time": "2025-04-06T15:10:27.259133Z"
    }
   },
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grimm\\PycharmProjects\\hand-gesture-recognition-mediapipe\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.306131Z",
     "start_time": "2025-04-06T15:10:27.291134Z"
    }
   },
   "source": [
    "# 入出力テンソルを取得\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.337021Z",
     "start_time": "2025-04-06T15:10:27.322449Z"
    }
   },
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.368247Z",
     "start_time": "2025-04-06T15:10:27.354023Z"
    }
   },
   "source": [
    "%%time\n",
    "# 推論実施\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.415239Z",
     "start_time": "2025-04-06T15:10:27.384549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 推論専用のモデルとして保存\n",
    "model.save(model_save_path, include_optimizer=False)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:10:27.445830Z",
     "start_time": "2025-04-06T15:10:27.431554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1954637e-02 7.1722366e-02 8.7705678e-01 8.6475220e-03 5.7122711e-04\n",
      " 4.7421348e-05]\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
